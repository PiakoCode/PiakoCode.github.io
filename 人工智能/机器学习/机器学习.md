## 回归

**最小二乘法**


均方误差

$$
f(x) = w \times x+b
$$
$$
error =\frac{1}{2m} \sum\limits_{i=1}^{m}(f(x) - y)^2
$$

**梯度下降**

$$
w = w - \alpha \frac{\partial}{\partial w}J(w,b) 
$$


$$
b = b - \alpha \frac{\partial}{\partial b}J(w,b)
$$


$$

\begin{align}

\frac{\partial J(w,b)}{\partial w} &= \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)} \\

\frac{\partial J(w,b)}{\partial b} &= \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) 

\end{align}

$$


**逻辑回归模型**

$$
\begin{align}

z = w \cdot x + b \\

g(z) = \frac{1}{1+e^{-z}}
\end{align}
$$

**交叉熵损失函数(Cross-Entropy Loss)**


1. 二分类

对于每个类别我们预测得到的概率为 $p$ 和 $1-p$ 
$$
L = \frac{1}{N}\sum\limits^{N}_{i}L_i=-\frac{1}{N}\sum\limits^{N}_{i}[y_i \cdot log(p_i)+(1-y_i)\cdot log(1-p_i)]
$$

- $y_i$ —— 表示样本 $i$ 的label，正类为$1$，负类为$0$
- $p_i$ —— 表示样本 $i$ 预测为正类的概率

2. 多分类

$$
L = \frac{1}{N}\sum\limits_{i}L_i = -\frac{1}{N}\sum\limits_{i}\sum\limits_{c=1}^{M}y_{ic}log(p_{ic})
$$

- $M$ —— 类别的数量
- $y_{ic}$ —— 符号函数（0 或 1）
- $p_{ic}$ —— 观测样本 $i$ 属于类别 $c$ 的预测概率

 **正则化**

正则化用于防止模型过拟合训练数据。
在正则化过程中，通常会添加一个 **正则化项** 到模型的损失函数中。

L1正则化（Lasso正则化）:

L1正则化通过在损失函数中增加参数的绝对值之和来实现惩罚。它的效果是使得部分参数变为0，从而实现特征选择（即自动筛选出对目标变量最重要的特征）。

$$
\text{正则化项} = \lambda \sum_{i=1}^n |w_i|
$$

L2正则化（Ridge正则化）:

L2正则化通过在损失函数中增加参数的平方和来实现惩罚。它的效果是使得参数的取值尽量小，避免出现过大的参数值。

$$
\text{正则化项} = \lambda \sum_{i=1}^n w_i^2
$$

其中 $w_i$ 是模型的权重参数，$\lambda$ 是正则化强度（正则化系数）。


## 神经网络

输入层

输出层

隐藏层

***

**对于输出层的激活函数**

二分类 推荐使用sigmoid

线性回归问题 推荐不使用激活函数

不能出现负值 推荐使用ReLU


>[!info] 
>**线性函数的线性函数本身就是线性函数，所以需要激活函数引入非线性**
>如果没有激活函数，那么神经网络 *等价于* 线性回归


Softmax函数

$$
a_x = \frac{e^{z_x}}{e^{z_1} + e^{z_2} + \dots + e^{z_x} + \dots +e^{z_n} }
$$


Softmax回归

![](Picture/Pasted%20image%2020230716213002.png)

Adam Algorithm 
自动调整下降速率

卷积层

不能出现


***
训练集（training set）

交叉验证集（cross validation set | validation set）

测试集（test set）


交叉验证集在模型训练过程中使用，用于评估模型的性能和选择合适的参数，可以帮助改善模型的质量。

测试集在模型训练完成后使用，用于最终评估模型在未知数据上的表现，以验证模型的泛化能力。

## 决策树

熵函数计算

$$
p_0 = 1 - p_1
$$
$$
H(p_1) = - p_1log_2(p_1) - p_0log_2(p_0)
$$

设置"0log(0)" = 0

![](Picture/Pasted%20image%2020230718213005.png)

计算熵的减少(信息的增益)

Information gain
$$
 = H(p_1^{root}) - (w^{left}H(p_1^{left}) + w^{right}H(p_1^{right})) 
$$

独热编码 one-hot


回归树

计算方差的减少（选择分类的选项，计算回归）

![](Picture/Pasted%20image%2020230719160923.png)

使用多个决策树，使算法更加健壮。


### 随机森林

1. 一个样本容量为N的样本，**有放回的抽取** N次，每次抽取1个，最终形成了N个样本。用这些选择好的N个样本用来训练一个决策树，作为决策树根节点处的样本。

2. 当每个样本有M个属性时，在决策树的每个节点需要分裂时，随机从这M个属性中选取出m个属性，满足条件m << M。然后从这m个属性中采用某种策略（比如说信息增益）来选择1个属性作为该节点的分裂属性。

3. 决策树形成过程中每个节点都要按照步骤2来分裂（很容易理解，如果下一次该节点选出来的那一个属性是刚刚其父节点分裂时用过的属性，则该节点已经达到了叶子节点，无须继续分裂了）。一直到不能够再分裂为止。注意整个决策树形成过程中没有进行剪枝。

4. 按照步骤1~3建立大量的决策树，这样就构成了随机森林了。

随机森林模型有四个主要优点：

1. 非常适合回归和分类问题。回归中的输出变量是一个数字序列，例如某个街区的房价。分类问题的输出变量通常是一个单一答案，例如房屋的售价是否高于或低于要价。
2. 可以处理缺失值并保持高准确性，即使由于 bagging 和有放回抽样而缺失大量数据时也是如此。
3. 算法由于输出的是“多数规则”，使得模型几乎不可能过拟合。
4. 该模型可以处理包含数千个输入变量的庞大数据集，因此成为降维的不错工具。
5. 其算法可用于从训练数据集中识别非常重要的特征。

其也有一些缺点：

1. 随机森林优于决策树，但其准确性低于 XGBoost 等梯度提升树集成。
2. 随机森林包含大量树，因此速度比 XGBoost 慢。


>让我们以一个找工作的比喻来解释随机森林的原理。
>
>假设你正在寻找一份新的工作，你决定向多个朋友寻求建议。每个朋友都是一位专家，他们有自己独特的经验和观点。你决定根据他们的建议来做出最终的决定。
>
>在随机森林中，每个决策树就像是一个朋友，它们基于不同的特征和规则来做出预测。每个决策树都是独立构建的，它们之间没有关联。这种随机性使得每个决策树都有自己的观点和预测结果。
>
>假设你的朋友们为你提供了各自的建议，你可以根据他们的建议做出最终的决策。在随机森林中，我们通过投票或平均的方式来汇总每个决策树的预测结果，从而得到集成分类器的最终预测结果。
>
>这种集成的方式可以提高模型的准确性和鲁棒性。就像在找工作时，如果你得到了多个专家的建议，你会更有信心做出正确的决策。


### XGBoots


## 聚类

### K-means

步骤：

1. 选择 K：确定要分成的簇的数量 K。这可以是预先设定的值，也可以通过一些评估指标或调参方法来确定。
    
2. 随机初始化聚类中心：从数据集中随机选择 K 个数据点作为初始的聚类中心。
    
3. 分配数据点到最近的聚类中心：对于每个数据点，计算其与每个聚类中心的距离，并将其分配给距离最近的聚类中心。
    
4. 更新聚类中心：对于每个簇，计算簇内所有数据点的平均值（或中心点），并将这个平均值作为新的聚类中心。
    
5. 重复步骤 3 和 4：重复执行步骤 3 和 4，直到聚类中心不再发生明显的变化，或者达到预定的迭代次数。

代码

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans

# 生成示例数据
X, y = make_blobs(n_samples=1000, centers=3, random_state=42, cluster_std=1.0)

# 可视化数据点
plt.scatter(X[:, 0], X[:, 1], s=10)
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.title("Data points")
plt.show()

    
# 使用K-means算法进行聚类
kmeans = KMeans(n_clusters=3, random_state=42)
kmeans.fit(X)

# 获取聚类中心和聚类标签
centers = kmeans.cluster_centers_
labels = kmeans.labels_

# 可视化聚类结果
plt.scatter(X[:, 0], X[:, 1], c=labels, s=10, cmap='viridis')
plt.scatter(centers[:, 0], centers[:, 1], c='red', s=100, marker='X', label='Centroids')
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.title("K-means Clustering")
plt.legend()
plt.show()
```

![](Picture/Pasted%20image%2020230903161330.png)









***


## Rewrite



