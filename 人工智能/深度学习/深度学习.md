# 深度学习

#人工智能 #深度学习


## 线性神经网络

### 线性回归

线性回归是一种基本的回归模型，用于建立输入变量和输出变量之间的线性关系。它是一种最简单的神经网络模型，只包含一个输入层和一个输出层，其中输入层的每个神经元对应一个输入变量，输出层的一个神经元对应一个输出变量。

线性回归模型的输出是输入变量的线性组合，通过乘以相应的权重并加上偏置项来实现。这可以表示为：

y = w1\*x1 + w2\*x2 + ... + wn\*xn + b

其中，y是输出变量，x1, x2, ..., xn是输入变量，w1, w2, ..., wn是对应的权重，b是偏置项。

在神经网络中，线性回归通常作为神经网络中的一层，其中的权重和偏置项是需要通过训练来学习得到的。通过反向传播算法和优化算法（如梯度下降），可以根据给定的训练数据来调整权重和偏置项，使得模型能够更好地拟合输入和输出之间的线性关系。

线性回归在神经网络中常用于解决回归问题，如预测房价、销售额等连续变量的问题。它的优点是计算简单、易于理解，但对于非线性关系的建模能力有限。在处理非线性问题时，可以通过增加隐藏层和激活函数来构建更复杂的神经网络模型。


### softmax


Softmax函数是一种常用的激活函数，它将一组实数转化为概率分布。在机器学习中，它通常用于多分类问题中，将原始的实数输出转化为每个类别的概率。

Softmax函数的定义如下：
$$
\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}
$$
其中，$z_i$是输入向量中的第$i$个元素，$K$是类别的总数。

Softmax函数的特点是它对输入进行归一化，使得所有输出的概率之和为1。它将原始的实数转化为概率，较大的输入会得到较大的输出概率，而较小的输入会得到较小的输出概率。

在深度学习中，Softmax函数通常与交叉熵损失函数结合使用，用于计算模型的预测和真实标签之间的差距。
